# Architectural Basics

## 1. 3x3 Convolutions
## 2. Receptive Field
## 3. Kernels and how do we decide the number of kernels?
## 4. How many layers
## 5. How do we know our network is not going well, comparatively, very early
## 6. MaxPooling
## 7. Position of MaxPooling
## 8. The distance of MaxPooling from Prediction
## 9. 1x1 Convolutions
## 10. Concept of Transition Layers
## 11. Position of Transition Layer
## 12. SoftMax
## 13. Dense Layers or FC Layers
## 14. When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)
## 14.5 Over fitting
## 15. Image Normalization
## 16. Batch Normalization
## 17. The distance of Batch Normalization from Prediction
## 18. DropOut
## 19. When do we introduce DropOut, or when do we know we have some overfitting
## 20. Batch Size, and effects of batch size
## 21. Number of Epochs and when to increase them
## 22. When to add validation checks
## 23. Learning Rate
## 24. LR schedule and concept behind it
## 25. Adam vs SGD
## 26. Forward pass, the loss function, the backward pass, and the weight update
