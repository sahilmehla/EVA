# Write your Assignment 3 again such that:
### 1. Finally after 4 code iterations achieve 99.4% accuracy.
### 2. Less than 15k Parameters.
### 3. Have started from a Vanilla network (no BN, DropOut, LR, larger batch size, change in Optimizer, etc).
### 4. Make sure you are tracking your code's performance, and writing down your observations are you achieve better or worse results
### 5. Your second code can only have max 3 improvements over first one, third can have only max 3 over second and so on. 
### 6. All of your iterations are in different files and named properly like First, Second, etc.
### 7. All of you iterations are in a single folder
### 8. All of your iterations have a Header note, describing what all you are planning to do in this code
### 19. All of your code is very well documented
### 20. There is a readme file describing the below mentioned (at least 24) points. The title of this file is Architectural Basics. You need to put them in order in which you will think about them or execute them in your experiments thought behind that order

- How many layers,
- MaxPooling,
- 1x1 Convolutions,
- 3x3 Convolutions,
- Receptive Field,
- SoftMax,
- Learning Rate,
- Kernels and how do we decide the number of kernels?
- Batch Normalization,
- Image Normalization,
- Position of MaxPooling,
- Concept of Transition Layers,
- Position of Transition Layer,
- Number of Epochs and when to increase them,
- DropOut
- When do we introduce DropOut, or when do we know we have some overfitting
- The distance of MaxPooling from Prediction,
- The distance of Batch Normalization from Prediction,
- When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)
- How do we know our network is not going well, comparatively, very early
- Batch Size, and effects of batch size
- When to add validation checks
- LR schedule and concept behind it
- Adam vs SGD
- etc (you can add more if we missed it here)
