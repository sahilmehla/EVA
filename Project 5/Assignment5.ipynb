{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "- **Importing keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRS90KkL9JqN",
        "colab_type": "text"
      },
      "source": [
        "- **Importing models and layers from keras**\n",
        "- **Importing mnist dataset**\n",
        "- **Importing python utilities - numpy**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "- **Load pre-shuffled MNIST data into train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqt13Wi-4Pj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- **Printing a image from the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "d16bb3a1-b030-48ba-a6a9-d2291a8a1a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7e3cd10f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0jrqIYx_G0U",
        "colab_type": "text"
      },
      "source": [
        "- **Reshaping the images as 28x28 matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UchoEqV_PVE",
        "colab_type": "text"
      },
      "source": [
        "- **Normalizing both the training and test images\n",
        "such that each value in the image will be b/w 0.0 and 1.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKLK_sDS_e7f",
        "colab_type": "text"
      },
      "source": [
        "- **Listing the labels (from 0 to 9) for first 10 images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "39c2de3d-ee57-4b4e-b398-9d5b52acc1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc0b9Vus_kNQ",
        "colab_type": "text"
      },
      "source": [
        "- **Convert 1-dimensional class arrays to 10-dimensional class matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBuyGiwe_7BJ",
        "colab_type": "text"
      },
      "source": [
        "- **This is how the 10 Dimentional class matrix looks like\n",
        " The position of \"1.\" at each row is basically the label\n",
        " of the image.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "2804ebb6-d540-4be7-e87d-a6776a4a57e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXW6AvDtC9sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Image Normalization\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "datagen.fit(X_train)\n",
        "training_set= datagen.flow(X_train, Y_train, batch_size=128)\n",
        "test_set= datagen.flow(X_test, Y_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5bMWyrxAFNs",
        "colab_type": "text"
      },
      "source": [
        "## Model with \"batch normalization before ReLu\" And \"L2 Regularization\"\n",
        "              \n",
        " \n",
        "              \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "43937006-61b4-4b23-c673-5d51c703dd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "#INPUT LAYER                       # Output channel dimensions | Receptive field\n",
        "model.add(Conv2D(16, 3, 3, input_shape=(28,28,1))) # 26 | 3x3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "# CONVOLUTIONAL LAYER\n",
        "model.add(Conv2D(16, (3, 3),kernel_regularizer=l2(0.01))) # 24 | 5x5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "#TRANSITION LAYER\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 12 | 10x10\n",
        "model.add(Conv2D(10, (1, 1), kernel_regularizer=l2(0.01))) # 12\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#CONVOLUTIONAL LAYER\n",
        "model.add(Conv2D(16, (3, 3),kernel_regularizer=l2(0.01))) #  10 | 12x12\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Conv2D(16, (3, 3), kernel_regularizer=l2(0.01))) #   8 | 14x14\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Conv2D(16, (3, 3), kernel_regularizer=l2(0.01))) #   6 | 16x16\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Conv2D(16, (3, 3), kernel_regularizer=l2(0.01))) #   4 | 18x18\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "#OUTPUT LAYER\n",
        "model.add(Conv2D(10, 1, activation='relu')) #4\n",
        "model.add(Conv2D(10, 4))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "170c5966-9caf-4af0-8ac8-0e5a064993f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1055
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_114 (Conv2D)          (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_98 (Activation)   (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_115 (Conv2D)          (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_99 (Activation)   (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_116 (Conv2D)          (None, 12, 12, 10)        170       \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_100 (Activation)  (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_117 (Conv2D)          (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_101 (Activation)  (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_118 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_102 (Activation)  (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_119 (Conv2D)          (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_103 (Activation)  (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_120 (Conv2D)          (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_104 (Activation)  (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_121 (Conv2D)          (None, 4, 4, 10)          170       \n",
            "_________________________________________________________________\n",
            "conv2d_122 (Conv2D)          (None, 1, 1, 10)          1610      \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_105 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,270\n",
            "Trainable params: 13,058\n",
            "Non-trainable params: 212\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsXLiI6BAacS",
        "colab_type": "text"
      },
      "source": [
        "- **Using stochastic gradient descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWJWlRu1ASZ_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- **Training the model with batch size of 128 images and epoch = 40**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "69e192bb-8095-40a3-b0dd-66749adbeed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1372
        }
      },
      "source": [
        "# Saving the model with best validation accuracy\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# fit model with generator\n",
        "model.fit_generator(training_set, steps_per_epoch=len(training_set), epochs=40, validation_data = test_set, validation_steps = len(test_set), callbacks=[earlyStopping, mcp_save])\n"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "469/469 [==============================] - 14s 29ms/step - loss: 0.5145 - acc: 0.9295 - val_loss: 0.7762 - val_acc: 0.7659\n",
            "Epoch 2/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1804 - acc: 0.9728 - val_loss: 0.2285 - val_acc: 0.9534\n",
            "Epoch 3/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1548 - acc: 0.9761 - val_loss: 0.2295 - val_acc: 0.9480\n",
            "Epoch 4/40\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.1433 - acc: 0.9775 - val_loss: 0.2297 - val_acc: 0.9455\n",
            "Epoch 5/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1414 - acc: 0.9768 - val_loss: 0.1318 - val_acc: 0.9788\n",
            "Epoch 6/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1339 - acc: 0.9788 - val_loss: 0.2536 - val_acc: 0.9418\n",
            "Epoch 7/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1299 - acc: 0.9795 - val_loss: 0.1326 - val_acc: 0.9780\n",
            "Epoch 8/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1281 - acc: 0.9786 - val_loss: 0.1571 - val_acc: 0.9694\n",
            "Epoch 9/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1236 - acc: 0.9797 - val_loss: 0.1488 - val_acc: 0.9742\n",
            "Epoch 10/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1191 - acc: 0.9805 - val_loss: 0.1328 - val_acc: 0.9766\n",
            "Epoch 11/40\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.1184 - acc: 0.9803 - val_loss: 0.1668 - val_acc: 0.9641\n",
            "Epoch 12/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1119 - acc: 0.9813 - val_loss: 0.1395 - val_acc: 0.9714\n",
            "Epoch 13/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1142 - acc: 0.9800 - val_loss: 0.1603 - val_acc: 0.9633\n",
            "Epoch 14/40\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.1113 - acc: 0.9817 - val_loss: 0.1472 - val_acc: 0.9667\n",
            "Epoch 15/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1099 - acc: 0.9820 - val_loss: 0.1032 - val_acc: 0.9832\n",
            "Epoch 16/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1069 - acc: 0.9822 - val_loss: 0.2336 - val_acc: 0.9356\n",
            "Epoch 17/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1075 - acc: 0.9814 - val_loss: 0.1622 - val_acc: 0.9612\n",
            "Epoch 18/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1047 - acc: 0.9821 - val_loss: 0.1052 - val_acc: 0.9807\n",
            "Epoch 19/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1042 - acc: 0.9820 - val_loss: 0.1004 - val_acc: 0.9839\n",
            "Epoch 20/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.1017 - acc: 0.9828 - val_loss: 0.1325 - val_acc: 0.9740\n",
            "Epoch 21/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0979 - acc: 0.9832 - val_loss: 0.2126 - val_acc: 0.9477\n",
            "Epoch 22/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0998 - acc: 0.9826 - val_loss: 0.1119 - val_acc: 0.9780\n",
            "Epoch 23/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0976 - acc: 0.9827 - val_loss: 0.1342 - val_acc: 0.9702\n",
            "Epoch 24/40\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0952 - acc: 0.9831 - val_loss: 0.1193 - val_acc: 0.9765\n",
            "Epoch 25/40\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0927 - acc: 0.9838 - val_loss: 0.1279 - val_acc: 0.9713\n",
            "Epoch 26/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0963 - acc: 0.9827 - val_loss: 0.1278 - val_acc: 0.9717\n",
            "Epoch 27/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0938 - acc: 0.9831 - val_loss: 0.1078 - val_acc: 0.9797\n",
            "Epoch 28/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0949 - acc: 0.9827 - val_loss: 0.0985 - val_acc: 0.9818\n",
            "Epoch 29/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0955 - acc: 0.9833 - val_loss: 0.1029 - val_acc: 0.9787\n",
            "Epoch 30/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0916 - acc: 0.9832 - val_loss: 0.1157 - val_acc: 0.9784\n",
            "Epoch 31/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0937 - acc: 0.9833 - val_loss: 0.1209 - val_acc: 0.9744\n",
            "Epoch 32/40\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0911 - acc: 0.9834 - val_loss: 0.1053 - val_acc: 0.9796\n",
            "Epoch 33/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0919 - acc: 0.9839 - val_loss: 0.1136 - val_acc: 0.9750\n",
            "Epoch 34/40\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0895 - acc: 0.9839 - val_loss: 0.1009 - val_acc: 0.9805\n",
            "Epoch 35/40\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0915 - acc: 0.9833 - val_loss: 0.1269 - val_acc: 0.9726\n",
            "Epoch 36/40\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0909 - acc: 0.9836 - val_loss: 0.1880 - val_acc: 0.9528\n",
            "Epoch 37/40\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0897 - acc: 0.9842 - val_loss: 0.1326 - val_acc: 0.9705\n",
            "Epoch 38/40\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0922 - acc: 0.9837 - val_loss: 0.1394 - val_acc: 0.9693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e3fd829b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdd87dac-d5b5-4a0e-b590-4748ffd86acf"
      },
      "source": [
        "\n",
        "# evaluate model\n",
        "_, acc = model.evaluate_generator(test_set, steps=len(test_set), verbose=0)\n",
        "print('Test Accuracy: %.3f' % (acc * 100))"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 96.990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgdkBzSlBh-S",
        "colab_type": "text"
      },
      "source": [
        "- **Finding 25 misclassified images from the validation dataset and creating an image gallery**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(y_pred[:9])\n",
        "#print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIH59IOGC6iP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5508
        },
        "outputId": "b090deb6-2ebb-4c23-f987-d4b846ceb097"
      },
      "source": [
        "#predicted_classes = model.predict_classes(X_test)\n",
        "\n",
        "\n",
        "\n",
        "#test_set.reset()\n",
        "y_pred = model.predict_generator(test_set, steps=10000, verbose=0)\n",
        "y_classes = y_pred.argmax(axis=-1)\n",
        "print(y_pred[:100])\n",
        "print(y_classes[:100])\n",
        "print(y_test[:100])\n",
        "\n",
        "#See which we predicted correctly and incorrectly\n",
        "correct_indices = np.nonzero(y_classes == y_test)[0]\n",
        "incorrect_indices = np.nonzero(y_classes != y_test)[0] \n",
        "print(len(correct_indices),\" classified correctly\") \n",
        "print(len(incorrect_indices),\" classified incorrectly\")\n",
        "\n",
        "#result = np.absolute(Y_test - newy_pred)\n",
        "#print(result[:100])\n"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.81268324e-08 2.27095440e-10 1.13444196e-06 3.34526362e-08\n",
            "  1.14819595e-05 1.22070310e-06 4.31771277e-08 1.75095920e-06\n",
            "  9.99984264e-01 1.35782550e-08]\n",
            " [1.53876811e-01 2.09827203e-06 8.28786969e-01 6.38148142e-03\n",
            "  2.10271105e-06 1.68080169e-05 4.57930983e-05 6.11949781e-06\n",
            "  1.07596144e-02 1.22244499e-04]\n",
            " [2.40800148e-07 6.72807857e-07 2.17929355e-07 4.15785878e-04\n",
            "  2.82792723e-08 9.99511123e-01 1.24600149e-07 2.44539478e-07\n",
            "  8.51094956e-06 6.30408322e-05]\n",
            " [5.94939763e-07 9.98460889e-01 5.42354363e-04 1.02575746e-06\n",
            "  8.67198978e-05 1.08553695e-06 4.97103383e-06 8.76963662e-04\n",
            "  2.39288947e-05 1.50473772e-06]\n",
            " [3.88578414e-09 8.41730852e-11 1.62773904e-06 1.00019236e-06\n",
            "  3.83821060e-07 1.07489903e-07 1.91304395e-08 1.59406045e-06\n",
            "  9.99995232e-01 1.17715668e-08]\n",
            " [7.32615704e-11 3.18998272e-10 2.06076621e-07 9.99997854e-01\n",
            "  9.44024589e-14 9.11307385e-09 7.98379443e-14 1.37831194e-06\n",
            "  4.32072284e-07 1.43189971e-09]\n",
            " [4.51369579e-06 7.61311591e-01 2.37849072e-01 3.12099728e-05\n",
            "  5.70346368e-04 5.33602417e-07 7.66217545e-06 1.96035820e-04\n",
            "  2.89000673e-05 6.66690525e-08]\n",
            " [5.56356383e-07 1.08114746e-05 6.88480895e-07 1.21006067e-03\n",
            "  1.10304381e-06 9.98408377e-01 3.15837838e-07 1.10611509e-05\n",
            "  2.90272783e-05 3.28006077e-04]\n",
            " [8.43805310e-07 5.44653062e-07 3.17456252e-05 6.10341555e-09\n",
            "  6.78719607e-06 1.80913376e-05 9.99940872e-01 1.64754654e-10\n",
            "  1.07685617e-06 3.17237681e-09]\n",
            " [9.33412637e-04 5.79354295e-04 5.08153287e-04 4.02338410e-05\n",
            "  6.27140344e-06 9.06017363e-01 8.41022655e-02 1.16096060e-04\n",
            "  6.90906867e-03 7.87712983e-04]\n",
            " [2.92295654e-09 3.57435852e-07 4.71765134e-06 3.14351325e-07\n",
            "  8.23459959e-06 5.92427902e-08 1.05875739e-14 9.99939799e-01\n",
            "  3.92230213e-08 4.64330042e-05]\n",
            " [1.08329354e-04 2.66901596e-04 1.47898332e-04 1.00169266e-02\n",
            "  3.96584067e-03 1.43201905e-04 2.64841844e-07 6.70886636e-01\n",
            "  3.09181452e-01 5.28254360e-03]\n",
            " [1.00808730e-08 7.51057314e-08 6.50251852e-10 1.11488320e-04\n",
            "  1.83956494e-09 9.99884009e-01 3.72610165e-09 3.91255278e-10\n",
            "  1.82667657e-06 2.62142885e-06]\n",
            " [8.24505531e-09 1.60115314e-06 1.20200148e-05 9.97990847e-01\n",
            "  8.57458460e-08 4.95693712e-06 7.17489978e-13 1.96871418e-03\n",
            "  2.70210791e-07 2.15328964e-05]\n",
            " [8.41136796e-08 3.10457017e-06 1.11048486e-07 1.10560453e-04\n",
            "  2.67961525e-07 9.99575794e-01 3.10036397e-09 3.72371858e-08\n",
            "  1.40577122e-05 2.95957230e-04]\n",
            " [3.00282643e-09 1.65420360e-07 6.74683761e-05 3.62430280e-10\n",
            "  9.99916792e-01 3.74476228e-09 1.06923570e-09 8.39507663e-09\n",
            "  2.09904041e-08 1.56022652e-05]\n",
            " [2.65087874e-04 6.25586338e-08 1.66977279e-05 2.65943925e-08\n",
            "  2.75214717e-07 5.03158243e-03 9.94613469e-01 1.32509336e-12\n",
            "  7.00186429e-05 2.76680680e-06]\n",
            " [5.28082683e-06 1.76767987e-08 1.34378325e-07 4.79666173e-09\n",
            "  3.67511660e-07 4.44353600e-06 9.99973655e-01 1.11414892e-13\n",
            "  1.60550426e-05 1.76246417e-09]\n",
            " [9.94407177e-01 2.86803811e-07 7.47222744e-04 8.20586574e-05\n",
            "  9.23831351e-07 1.01154483e-05 2.02260367e-06 4.49147448e-03\n",
            "  9.71871996e-05 1.61551259e-04]\n",
            " [2.39449741e-06 1.62129538e-04 9.99309659e-01 5.61522529e-06\n",
            "  3.46829329e-04 4.78744653e-07 1.18909341e-07 2.25557437e-06\n",
            "  1.70480664e-04 7.30475165e-08]\n",
            " [4.31441842e-03 4.14718343e-05 6.19383482e-03 5.74614241e-05\n",
            "  1.28403321e-01 5.78899577e-04 7.91114726e-05 3.23763979e-03\n",
            "  9.39982105e-03 8.47694039e-01]\n",
            " [1.60012412e-08 2.15703122e-10 2.01167904e-06 1.31319450e-06\n",
            "  7.35803640e-10 2.69856848e-08 2.67585904e-07 6.74207287e-11\n",
            "  9.99996305e-01 1.14028076e-09]\n",
            " [2.92093609e-04 7.73759652e-03 4.55860805e-04 4.83483542e-03\n",
            "  1.35967434e-06 9.80906072e-04 1.14577338e-01 8.21202661e-07\n",
            "  8.71119142e-01 1.15728369e-07]\n",
            " [9.99371231e-01 7.67578712e-08 2.28135657e-04 6.06869571e-06\n",
            "  3.22827123e-06 5.05032540e-06 2.60619883e-04 3.50365212e-07\n",
            "  1.07667292e-04 1.75297337e-05]\n",
            " [1.83713040e-04 1.19005581e-05 2.77467631e-03 1.10046205e-09\n",
            "  9.96914029e-01 5.35960112e-07 5.09781130e-05 7.28566283e-06\n",
            "  4.94746291e-06 5.18688285e-05]\n",
            " [5.33061595e-08 1.91869773e-08 6.24600034e-07 7.79162690e-09\n",
            "  3.54548234e-07 3.99387545e-06 9.99993682e-01 1.24523655e-11\n",
            "  1.34981428e-06 2.84176682e-10]\n",
            " [2.61288822e-08 3.94926802e-09 1.25008701e-05 3.66593054e-06\n",
            "  1.37695588e-06 5.72409306e-07 3.01566752e-06 1.08400194e-07\n",
            "  9.99978662e-01 6.67956499e-08]\n",
            " [2.13542140e-09 2.23893148e-05 7.99422996e-05 1.01127398e-06\n",
            "  3.01418686e-06 5.71985552e-08 1.18326600e-13 9.99866128e-01\n",
            "  9.74099947e-08 2.73264741e-05]\n",
            " [2.56742343e-08 9.97939885e-01 1.76228338e-03 3.05372282e-07\n",
            "  1.18697513e-04 7.84365710e-08 2.34937360e-07 1.77274182e-04\n",
            "  9.87792077e-07 1.48364450e-07]\n",
            " [6.72185934e-08 4.22531237e-08 9.99195516e-01 5.64676244e-04\n",
            "  1.19010961e-08 1.00468323e-09 4.10354840e-11 2.14907064e-04\n",
            "  2.47756816e-05 4.44805863e-08]\n",
            " [9.65088069e-01 2.73157941e-09 6.85174228e-08 9.45506144e-06\n",
            "  4.37256764e-09 2.40572466e-04 3.45434658e-02 8.40317966e-08\n",
            "  1.17326868e-04 8.94125378e-07]\n",
            " [3.78314793e-01 2.11277870e-06 2.46697222e-04 9.39959544e-04\n",
            "  7.09830942e-07 3.67304776e-04 1.62477031e-01 5.46231504e-09\n",
            "  4.57640141e-01 1.13071928e-05]\n",
            " [6.38023630e-05 4.58486502e-05 7.69463222e-05 1.16923861e-01\n",
            "  9.86654777e-06 8.82443905e-01 2.17609602e-04 3.95181087e-06\n",
            "  1.74250119e-04 3.99308374e-05]\n",
            " [9.99916196e-01 1.62812641e-09 2.95439463e-06 1.59842202e-08\n",
            "  9.27440524e-08 4.40722090e-07 1.37415391e-05 2.76653509e-07\n",
            "  1.75182140e-05 4.87782308e-05]\n",
            " [1.82545392e-08 4.07308789e-07 2.22394174e-08 3.81158388e-05\n",
            "  4.06849701e-08 9.99865890e-01 7.58055758e-05 1.19563873e-10\n",
            "  1.05201116e-05 9.18594287e-06]\n",
            " [5.35290319e-08 9.95152116e-01 4.35409043e-03 1.40762347e-06\n",
            "  1.82195639e-04 2.18362700e-07 2.65422784e-07 3.07414535e-04\n",
            "  2.04610819e-06 2.16116931e-07]\n",
            " [9.37275502e-09 9.98610139e-01 9.07664944e-04 1.99138285e-07\n",
            "  6.04742745e-05 4.27669136e-08 1.08616071e-07 4.20492492e-04\n",
            "  6.90528509e-07 1.59777287e-07]\n",
            " [9.99806225e-01 4.83871503e-08 6.68786233e-05 5.67624774e-08\n",
            "  5.60866567e-07 3.66082311e-08 9.03095279e-05 7.55573115e-09\n",
            "  1.88919785e-05 1.69863561e-05]\n",
            " [2.23698164e-03 2.08275480e-07 2.97169493e-07 2.98698978e-07\n",
            "  9.63048592e-07 4.73496730e-05 9.97693717e-01 8.47732041e-12\n",
            "  2.00448885e-05 1.47288418e-07]\n",
            " [1.76977280e-10 4.58426694e-05 5.69976862e-07 1.63184577e-08\n",
            "  9.99914408e-01 2.02432133e-08 2.54189395e-11 7.85915381e-06\n",
            "  2.39330422e-08 3.11948606e-05]\n",
            " [3.56266767e-08 4.24924380e-08 4.04115526e-05 2.48501294e-07\n",
            "  5.06308254e-07 3.08355510e-08 3.65757678e-15 9.99925852e-01\n",
            "  3.01797947e-08 3.27938251e-05]\n",
            " [3.92850459e-04 9.60077671e-08 9.98791039e-01 5.76119695e-04\n",
            "  9.54884285e-07 2.16747765e-07 3.71049964e-07 1.89075109e-07\n",
            "  2.35928746e-04 2.25264148e-06]\n",
            " [3.38974337e-06 9.98761773e-01 1.93656160e-04 5.55577844e-06\n",
            "  1.45098777e-04 5.40266865e-06 3.62409155e-05 3.25664354e-04\n",
            "  5.21494716e-04 1.76092192e-06]\n",
            " [7.60439463e-08 9.68642439e-07 3.06856037e-07 9.93551135e-01\n",
            "  5.20646470e-10 6.43496960e-03 5.18448506e-10 1.23188840e-06\n",
            "  5.79023572e-06 5.44085469e-06]\n",
            " [5.90804702e-05 1.91140863e-08 2.85618034e-06 2.15293152e-08\n",
            "  3.89880870e-06 2.31424997e-06 9.99931574e-01 1.51655511e-11\n",
            "  2.85843527e-07 1.09258940e-08]\n",
            " [3.77437254e-06 8.24289164e-05 9.45210934e-01 3.51813547e-02\n",
            "  2.35524476e-05 1.07512381e-06 9.60844500e-06 1.56304415e-03\n",
            "  1.79232620e-02 9.66924631e-07]\n",
            " [9.84623492e-01 1.73472188e-06 1.15912706e-02 1.18949265e-06\n",
            "  4.07809603e-06 1.20222623e-07 5.11395638e-06 5.89189258e-05\n",
            "  6.31389965e-04 3.08269029e-03]\n",
            " [2.77383592e-08 3.40615394e-08 9.14860575e-05 2.07332996e-05\n",
            "  8.27456461e-05 3.14476597e-06 1.07506025e-07 1.08726290e-05\n",
            "  9.99788463e-01 2.44320427e-06]\n",
            " [3.83770959e-10 3.87939508e-03 2.01158546e-05 7.16800969e-06\n",
            "  9.95996714e-01 1.22997722e-06 6.48669962e-09 7.99650370e-05\n",
            "  6.70574263e-06 8.73393492e-06]\n",
            " [1.64669250e-11 2.51107668e-09 3.52688886e-07 6.56637849e-06\n",
            "  1.23933505e-05 2.04922003e-06 1.98190079e-07 4.94354936e-07\n",
            "  9.99977946e-01 6.34815134e-09]\n",
            " [1.38431697e-10 7.07902570e-10 1.71105825e-07 9.99999523e-01\n",
            "  2.54255970e-12 3.20062625e-08 8.18839896e-14 2.82282446e-08\n",
            "  1.95124045e-07 2.55504915e-08]\n",
            " [1.26990490e-05 9.12404152e-10 4.05542596e-05 1.30513035e-05\n",
            "  4.80794293e-09 5.26747340e-03 3.21571506e-03 3.97709643e-11\n",
            "  9.91449416e-01 1.01316186e-06]\n",
            " [1.44015133e-10 9.57550098e-12 1.00212489e-07 5.22188266e-08\n",
            "  2.96262992e-09 3.54051934e-07 4.45801662e-09 4.54273147e-11\n",
            "  9.99999523e-01 3.16469267e-10]\n",
            " [6.53316734e-10 2.48178821e-06 9.99989629e-01 4.03603826e-06\n",
            "  2.83886664e-07 2.82157742e-10 3.75875886e-10 2.59384456e-06\n",
            "  9.00461430e-07 1.25062871e-09]\n",
            " [1.72607165e-10 1.77579494e-07 2.50319744e-07 9.99989152e-01\n",
            "  7.80457143e-10 3.00186298e-06 3.17498543e-14 3.91852927e-06\n",
            "  5.02281807e-08 3.53644168e-06]\n",
            " [7.69855035e-07 3.32472094e-09 1.07519702e-06 4.99081332e-10\n",
            "  6.08446442e-07 1.98032990e-06 9.99995232e-01 1.84895870e-12\n",
            "  1.78870124e-07 4.46511139e-09]\n",
            " [3.32327500e-06 5.18922377e-07 1.85252429e-05 1.92946991e-05\n",
            "  2.37709973e-02 6.87261490e-06 1.51047761e-08 4.64800559e-02\n",
            "  2.88237445e-03 9.26818073e-01]\n",
            " [1.17904949e-06 9.96291995e-01 3.34573188e-03 9.78217940e-07\n",
            "  6.68452703e-05 5.81985944e-07 9.35193930e-06 2.64429604e-04\n",
            "  1.85252993e-05 3.23861855e-07]\n",
            " [2.68380376e-08 2.10045937e-06 1.28500920e-04 6.49894173e-06\n",
            "  1.04134597e-06 3.92204953e-08 6.77010734e-15 9.99625564e-01\n",
            "  1.32651564e-06 2.34833497e-04]\n",
            " [2.84195423e-09 5.48560131e-09 2.62370715e-11 4.29970532e-06\n",
            "  2.28541763e-09 9.99982834e-01 7.72186981e-10 2.21900233e-12\n",
            "  1.85558605e-07 1.26709410e-05]\n",
            " [3.29065593e-08 3.37171688e-04 1.21123635e-03 1.61123662e-05\n",
            "  2.28696786e-06 3.26235892e-08 1.24056416e-11 9.98377681e-01\n",
            "  3.30484141e-07 5.51109370e-05]\n",
            " [2.73006826e-05 1.61461227e-04 2.30859104e-03 4.97578156e-09\n",
            "  9.89412963e-01 1.53385281e-05 8.04559235e-03 3.42436408e-07\n",
            "  2.72171701e-06 2.57040865e-05]\n",
            " [7.87342991e-09 1.89208365e-08 9.99983668e-01 1.62282072e-06\n",
            "  1.44744173e-11 1.17884082e-13 5.20734757e-12 1.24604157e-05\n",
            "  2.20163338e-06 1.63956591e-11]\n",
            " [9.98434007e-01 9.33018953e-07 1.45301141e-03 1.06626736e-07\n",
            "  9.74848490e-07 7.14245942e-08 5.52101556e-05 2.01890824e-08\n",
            "  3.04709065e-05 2.50538342e-05]\n",
            " [1.86776061e-09 4.71616979e-04 5.93946897e-04 1.95616664e-07\n",
            "  9.98925149e-01 8.64786955e-07 5.43430929e-07 4.57904343e-06\n",
            "  2.35419293e-07 2.88614956e-06]\n",
            " [4.79015272e-10 7.03154015e-08 4.80257913e-05 9.90916377e-11\n",
            "  9.99951363e-01 1.48596496e-10 1.09047227e-09 4.58580502e-07\n",
            "  1.20458479e-08 4.81231872e-08]\n",
            " [5.27362063e-05 1.49402499e-08 1.28516706e-06 4.58012295e-09\n",
            "  5.92861795e-07 3.37619031e-06 9.99940872e-01 4.94760103e-13\n",
            "  1.21809853e-06 6.76880241e-09]\n",
            " [1.07735113e-05 2.27067266e-02 4.80674316e-06 7.52954602e-01\n",
            "  6.34164337e-07 2.17542455e-01 2.00615705e-05 5.60862281e-06\n",
            "  6.28066203e-03 4.73614957e-04]\n",
            " [9.99760091e-01 9.00548827e-08 1.89744169e-05 5.32556783e-07\n",
            "  1.05073973e-07 1.41954206e-06 1.72738233e-04 6.87526779e-07\n",
            "  3.77937322e-05 7.48135653e-06]\n",
            " [4.54618121e-10 8.21650232e-08 6.10271798e-07 2.53353420e-07\n",
            "  2.10336038e-05 1.33096606e-07 8.63664609e-15 9.99943733e-01\n",
            "  2.43412686e-08 3.40120569e-05]\n",
            " [2.14629564e-11 8.71376324e-05 1.84808359e-05 5.09630489e-08\n",
            "  9.99891996e-01 5.98999428e-09 1.96527822e-10 2.03494096e-06\n",
            "  2.35620501e-08 2.13387253e-07]\n",
            " [5.17792418e-04 1.22224453e-10 4.20248796e-07 3.17193347e-08\n",
            "  1.59953345e-06 1.16670180e-05 9.99457419e-01 7.02073729e-12\n",
            "  1.10332130e-05 2.45749820e-08]\n",
            " [5.09536191e-10 3.44364288e-08 9.99984622e-01 1.27412322e-05\n",
            "  2.13996110e-08 7.86824599e-12 1.08698821e-10 5.73571697e-07\n",
            "  1.99978285e-06 8.43716243e-12]\n",
            " [3.60712590e-07 2.62032103e-07 1.18321999e-07 2.82878173e-04\n",
            "  6.35860886e-07 9.99534488e-01 5.45879448e-06 7.54559210e-07\n",
            "  4.15641989e-05 1.33547786e-04]\n",
            " [1.25453505e-03 9.15071845e-01 4.09286767e-02 9.87458261e-05\n",
            "  3.16995953e-04 9.13365875e-06 6.11953801e-05 3.48586291e-02\n",
            "  6.97614672e-03 4.24047117e-04]\n",
            " [1.16864676e-05 1.13311544e-04 6.23377552e-03 7.56585814e-06\n",
            "  9.49277759e-01 2.03918662e-05 4.14473973e-02 8.27675860e-04\n",
            "  2.05582916e-03 4.66537949e-06]\n",
            " [5.88985160e-03 4.01652933e-09 2.76340084e-04 1.13421184e-08\n",
            "  3.21974861e-03 7.46531668e-06 3.45976673e-06 7.79589027e-05\n",
            "  4.21123486e-03 9.86313820e-01]\n",
            " [9.99469340e-01 1.84233201e-07 3.44203290e-04 2.81799515e-07\n",
            "  2.44008135e-07 2.09563055e-07 3.34530500e-06 4.69524366e-06\n",
            "  1.44615347e-04 3.29717004e-05]\n",
            " [2.43461983e-07 5.38700851e-06 4.46920167e-05 2.28731452e-07\n",
            "  9.97984886e-01 1.55299276e-05 1.73290679e-03 2.01913707e-07\n",
            "  1.69637322e-04 4.61428936e-05]\n",
            " [9.98134971e-01 9.03933461e-09 1.46965531e-03 8.40988346e-09\n",
            "  2.96355302e-06 1.06525545e-06 1.39521333e-04 1.12171081e-08\n",
            "  1.94536100e-04 5.72910139e-05]\n",
            " [9.99659657e-01 2.69433912e-07 3.13231460e-04 3.13440580e-08\n",
            "  1.63050061e-07 1.01742765e-08 2.51149174e-07 3.54937697e-08\n",
            "  2.44945550e-05 1.89173056e-06]\n",
            " [5.86038273e-10 6.94216851e-10 9.05058580e-13 1.20350978e-05\n",
            "  2.28304736e-11 9.99985814e-01 1.57058877e-09 2.23033853e-12\n",
            "  3.34628325e-08 2.13847648e-06]\n",
            " [9.66180136e-10 2.25991997e-07 3.91251518e-07 9.99991059e-01\n",
            "  4.14150769e-10 3.48728668e-06 6.22531703e-13 2.83735108e-06\n",
            "  1.45471461e-06 5.58629722e-07]\n",
            " [1.71068559e-09 7.78876256e-06 9.99982119e-01 2.96718054e-06\n",
            "  4.02355454e-07 2.10636077e-08 3.75051656e-10 3.40495308e-06\n",
            "  3.31219439e-06 2.91667002e-09]\n",
            " [4.76324261e-04 6.01331962e-09 2.63052087e-08 3.33580346e-08\n",
            "  7.64585062e-10 6.19659387e-03 9.92632449e-01 8.41826236e-15\n",
            "  6.94313552e-04 1.34901640e-07]\n",
            " [6.06964434e-08 9.98107195e-01 2.95252568e-04 1.45263880e-06\n",
            "  1.38054922e-04 1.32808407e-06 8.35494745e-07 1.44923013e-03\n",
            "  4.94889900e-06 1.51502240e-06]\n",
            " [3.48058080e-07 1.50717568e-07 2.57835051e-08 6.51059236e-05\n",
            "  3.19274669e-08 9.99873042e-01 5.55679591e-08 1.13120988e-08\n",
            "  1.15196008e-06 6.00513631e-05]\n",
            " [1.26765609e-09 6.82809498e-09 4.11735783e-08 9.98541236e-01\n",
            "  5.29398886e-11 1.44044030e-03 9.70286460e-13 1.42175104e-05\n",
            "  3.13347414e-06 1.06097559e-06]\n",
            " [1.97115368e-09 5.03153252e-08 9.99843955e-01 1.45167083e-04\n",
            "  3.21343618e-09 1.93212030e-10 1.69867782e-08 2.83441590e-08\n",
            "  1.08513950e-05 1.40568626e-10]\n",
            " [3.55896077e-06 3.95103221e-08 7.59300656e-06 9.99982357e-01\n",
            "  7.72175934e-11 2.15109117e-06 1.37195227e-10 3.78411755e-06\n",
            "  4.84838438e-07 1.29487789e-08]\n",
            " [5.76946232e-03 5.19299839e-08 4.62125195e-03 4.26444598e-03\n",
            "  5.60722413e-07 2.34265113e-04 1.01403287e-02 1.16714500e-05\n",
            "  9.74939525e-01 1.83584434e-05]\n",
            " [1.11214002e-03 1.39957774e-06 1.27156716e-04 3.48424692e-05\n",
            "  4.28811262e-07 9.79289651e-01 2.75096274e-03 1.40576719e-06\n",
            "  1.66529082e-02 2.90685803e-05]\n",
            " [6.82015167e-10 2.21161429e-07 1.50031053e-06 9.99995470e-01\n",
            "  5.27698128e-11 2.55604647e-07 6.13851981e-12 2.07497669e-06\n",
            "  2.56776673e-07 1.40848471e-07]\n",
            " [1.09547386e-02 3.02729508e-11 2.41326452e-05 1.25336203e-06\n",
            "  3.10651167e-08 1.09871771e-05 1.64377252e-05 3.82233356e-09\n",
            "  9.88939464e-01 5.29669815e-05]\n",
            " [5.82625830e-07 9.93631661e-01 7.75845838e-04 8.71334396e-06\n",
            "  5.45197217e-05 4.22703124e-07 1.86800492e-06 5.45493234e-03\n",
            "  6.61057857e-05 5.37730784e-06]\n",
            " [1.82340798e-07 9.96974707e-01 4.18360025e-04 1.01641592e-06\n",
            "  1.34663540e-04 4.58786104e-07 2.67842051e-06 2.45658588e-03\n",
            "  9.41285816e-06 1.91020740e-06]\n",
            " [4.78712225e-09 7.29106091e-08 7.23678170e-07 9.99993205e-01\n",
            "  7.51320117e-11 1.04136541e-06 5.27211230e-14 2.96701523e-06\n",
            "  4.98707607e-07 1.41747023e-06]\n",
            " [4.20949391e-07 9.97227252e-01 2.55283178e-03 9.28502516e-07\n",
            "  4.08175256e-05 7.71989733e-07 1.28114716e-05 1.32480680e-04\n",
            "  3.15668549e-05 5.21675076e-08]\n",
            " [3.58991856e-05 1.31279463e-04 1.93837579e-04 8.70406075e-05\n",
            "  3.24367732e-03 2.08957528e-04 1.33820591e-04 2.95895134e-05\n",
            "  9.95881557e-01 5.44127543e-05]\n",
            " [1.96748829e-07 9.95761096e-01 3.54144373e-04 1.59030662e-06\n",
            "  1.50977299e-04 7.12741894e-07 1.93576011e-06 3.71218892e-03\n",
            "  1.41927667e-05 2.91804486e-06]]\n",
            "[8 2 5 1 8 3 1 5 6 5 7 7 5 3 5 4 6 6 0 2 9 8 8 0 4 6 8 7 1 2 0 8 5 0 5 1 1\n",
            " 0 6 4 7 2 1 3 6 2 0 8 4 8 3 8 8 2 3 6 9 1 7 5 7 4 2 0 4 4 6 3 0 7 4 6 2 5\n",
            " 1 4 9 0 4 0 0 5 3 2 6 1 5 3 2 3 8 5 3 8 1 1 3 1 8 1]\n",
            "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
            " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
            " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9]\n",
            "0  classified correctly\n",
            "1  classified incorrectly\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}